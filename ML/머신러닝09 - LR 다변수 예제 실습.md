# Logistic Regression 다변수 예제 실습

* 독립변수가 3개인 Logistic Regression 코드 구현 해보기.
* Python, Tensorflow, Sklearn으로 구현하고 Predict 비교해보기.





## 데이터 전처리

1. Raw 데이터 불러오기
2. 결측치 확인 및 제거
3. 이상치 확인 및 제거
4. 정규화 처리



* 코드구현 하기

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats  # 이상치 처리를 위해 필요
import tensorflow as tf  # tensorflow 구현
from sklearn import linear_model   # sklearn으로 simple linear regression 구현
from sklearn.preprocessing import MinMaxScaler  


# Raw data loading
df = pd.read_csv('./data/admission.csv')
# display(training_data)   # 확인 400 rows × 4 columns

# 결측치 처리
print(df.isnull().sum()) # 400 rows × 4 columns  # 결측치 없음

# 이상치 처리
fig = plt.figure()
fig_gre = fig.add_subplot(1,3,1)
fig_gpa = fig.add_subplot(1,3,2)
fig_rank = fig.add_subplot(1,3,3)

fig_gre.boxplot(df['gre'])
fig_gpa.boxplot(df['gpa'])
fig_rank.boxplot(df['rank'])

fig.tight_layout()
plt.show()

zscore_threshold = 2.0  # 2.0 이하로 설정하는게 좋음.

for col in df.columns:
    tmp = df[col][(np.abs(stats.zscore(df[col])) > zscore_threshold)]
    df = df.loc[~df[col].isin(tmp)]
    
    
# display(df)  # 382 rows × 4 columns


# normalization  t값은 0,1 이기 때문에 해줄 필요가 없음!!
# 여러 col을 때오면 dataframe으로 가져오기 때문에 values를 하면 2차원으로 가져옴 reshape은 필요가 없음 
x_data = df.drop('admit', axis=1, inplace=False).values
t_data = df['admit'].values.reshape(-1,1)

scaler_x = MinMaxScaler()  # 객체 생성
scaler_x.fit(x_data)
training_data_x = scaler_x.transform(x_data)
```



## Python 구현

```python
# python 구현

# 수치미분함수
def numerical_derivative(f,x):
    # f : 미분하려고 하는 다변수(W 1개, b 1개) 함수(loss 함수)
    # x : 모든 값을 포함하는 numpy array => [W, b]
    delta_x = 1e-4
    derivative_x = np.zeros_like(x)   # [0.0]  # 결과 저장용
    # np.zeros_like : ~처럼 만들어서 0으로 채우세요
    
    it = np.nditer(x, flags=['multi_index'])  # np array 반복할 때 사용 
    # flags를 사용하는 이유는 3개 이상으로 변수가 주어질 때 매트릭스로 들어 올 수 있기 때문
    
    while not it.finished:
        
        idx = it.multi_index  # 현재의 iterator의 index를 추출 => tuple형태로
      
        
        tmp = x[idx]  # 현재 index의 값을 잠시 보존. 
                      # delta_x를 이용한 값으로 ndarray를 수정한 후 편미분을 계산
                      # 함수값을 계산한 후 원상복구를 해 줘야 다음 독립변수에 대한
                      # 편미분을 정상적으로 수행할 수 있음.
        
        x[idx] = tmp + delta_x
        fx_plus_delta = f(x)   # f([1.00001, 2.0]) => f(x + delta_x)
        
        x[idx] = tmp - delta_x
        fx_minus_delta = f(x)   # f([0.99999, 2.0]) => f(x - delta_x)
        
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)
        
        x[idx] = tmp # 다음 독립변수 편미분을 위해 원상복귀
        
        it.iternext()
        
    return derivative_x


# Weight & bias
W = np.random.rand(3,1) # x의 열수 3 / t_data와 연산이 되야함. t_data의 col수와 동일
b = np.random.rand(1)  

# loss func
def loss_func(input_obj):  # input_obj : [w1 w2 w3 b]
    input_W = input_obj[:-1].reshape(-1,1)
    input_b = input_obj[-1:]
    
    z = np.dot(training_data_x, input_W) + input_b
    y = 1 / (1+ np.exp(-1*z))
    
    delta = 1e-7
    
    return -np.sum(t_data*np.log(y+delta) + (1-t_data)*np.log(1-y+delta))

# learing rate
learning_rate = 1e-4

# 학습(Gradient Descent Algorithm을 수행)
for step in range(300000):
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)   # [W1 W2 W3 b]
    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)
    
    W = W - derivative_result[:-1].reshape(-1,1)
    b = b - derivative_result[-1:]
    
    if step % 30000 == 0:
        input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)  # [W1 W2 W3 b]
        print('W : {}, b : {}, loss : {}'.format(W.ravel(),b,loss_func(input_param)))
```



* Python으로 Predict

```python
# python 결과

def logistic_predict(x):
    z = np.dot(x,W) + b
    y = 1 / (1 + np.exp(-1*z))
    
    if y < 0.5:
        result = 0
    else:
        result  = 1
    return result, y

my_score = np.array([600, 3.8, 1])
scaled_my_score = scaler_x.transform(my_score.reshape(-1,3))
python_result = logistic_predict(scaled_my_score)
print(python_result)
```

```
(1, array([[0.57333869]]))
```



## Tensorflow 구현

```python
# tensorflow 코드

# placeholder
X = tf.placeholder(shape=[None, 3], dtype=tf.float32)
T = tf.placeholder(shape=[None, 1], dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random.normal([3,1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')

# Hpyothesis
logit = tf.matmul(X,W) + b
H = tf.sigmoid(logit)

# loss function
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))

# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)

# Session & 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 학습
for step in range(300000):
    _, W_val, b_val, loss_val = sess.run([train, W, b, loss], 
                                         feed_dict={X: training_data_x, 
                                                    T : t_data})
    
    if step % 30000 == 0:
        print('W : {}, b : {}, loss : {}'.format(W_val.ravel(),b_val,loss_val))
```



* Tensorflow Predict

```python
# Tensor flow predict  600, 3.8, 1
predict_data = np.array([600, 3.8, 1])
scaled_predict_data = scaler_x.transform(predict_data.reshape(-1,3))
tensorflow_result = sess.run(H, feed_dict={X: scaled_predict_data})

print(tensorflow_result)
```

```
[[0.43410608]]
```



## Sklearn 구현

```python
# sklearn 구현

model = linear_model.LogisticRegression()

model.fit(x_data, t_data.ravel())
```



* Sklearn predict

```python
predict_data = np.array([[600, 3.8, 1]])
print(model.predict(predict_data))

result_pro = model.predict_proba(predict_data)  
print(result_pro)
```

```
[1]
[[0.43740782 0.56259218]]
```



# 오답노트

1. sklearn과 python, tensorflow에 쓰일 데이터를 구분하는 이상치 처리를 하지 않음.

   ```python
   # sklearn에 쓰일 데이터
   x_data = df.drop('admit', axis=1, inplace=False).values
   t_data = df['admit'].values.reshape(-1,1)
   
   # python, tensorflow에 쓰일 데이터 
   scaler_x = MinMaxScaler()  # 객체 생성
   scaler_x.fit(x_data)
   training_data_x = scaler_x.transform(x_data)
   ```

   

2. 결측치 처리 전에 isnull 을 사용해서 확인 먼저해주기.

   ```python
   print(df.isnull().sum())
   ```

   

3. 이상치 처리 하기 전에 boxplot을 이용해서 확인 먼저 해주기.

   ```python
   # plot 절차는 외우자!
   fig = plt.figure()
   fig_gre = fig.add_subplot(1,3,1)
   fig_gpa = fig.add_subplot(1,3,2)
   fig_rank = fig.add_subplot(1,3,3)
   
   fig_gre.boxplot(df['gre'])
   fig_gpa.boxplot(df['gpa'])
   fig_rank.boxplot(df['rank'])
   
   fig.tight_layout()
   plt.show()
   ```

   

4.  boolean indexing을 이용한 for문 이해하기.

   ```python
   for col in df.columns:
       tmp = df[col][(np.abs(stats.zscore(df[col])) > zscore_threshold)]
       df = df.loc[~df[col].isin(tmp)]
   ```



5. python 코드구현 시, loss function에서 input_W 부분 중 input_obj가 [w1, w2, w3, b]이기 때문에 slicing 해서 w인자들을  떼어와야한다. 그래서 코드구현 시, input_obj[:-1]가 된다.

   ```python
   for step in range(300000):
       input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)   # [W1 W2 W3 b]
       derivative_result = learning_rate * numerical_derivative(loss_func, input_param)
       
       W = W - derivative_result[:-1].reshape(-1,1)
       b = b - derivative_result[-1:]
   ```

   



# 한계점

1. w,b가 random으로 초기값 설정이 되어 결과에 영향을 미침.
2. 실제 정답을 모름.. sklearn으로 정답을 확인하는 중..성능평가요소가 부재.

